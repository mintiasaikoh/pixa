#!/usr/bin/env python3
"""
Pixa - æœ€é©åŒ–ç‰ˆã‚µãƒ¼ãƒãƒ¼å®Ÿè£…
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨èµ·å‹•æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›
"""
import os
import sys
import gc
import torch

# æœ€é©åŒ–1: é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
def lazy_import():
    """å¿…è¦ãªæ™‚ã ã‘ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹"""
    global Image, ImageFilter, ImageEnhance
    global StableDiffusionPipeline, DiffusionPipeline
    global Flask, request, jsonify, CORS
    
    from PIL import Image, ImageFilter, ImageEnhance
    from diffusers import StableDiffusionPipeline, DiffusionPipeline
    from flask import Flask, request, jsonify
    from flask_cors import CORS
    
# æœ€é©åŒ–2: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè¨­å®š
OPTIMIZATION_CONFIG = {
    'enable_attention_slicing': True,
    'enable_vae_slicing': True,
    'enable_sequential_cpu_offload': False,  # M2 Proã§ã¯ä¸è¦
    'enable_model_cpu_offload': False,       # M2 Proã§ã¯ä¸è¦
    'use_torch_compile': True,               # PyTorch 2.0+ã®æœ€é©åŒ–
    'use_channels_last': True,               # ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–
    'enable_xformers': True,                 # å¯èƒ½ãªå ´åˆæœ‰åŠ¹åŒ–
}

# æœ€é©åŒ–3: ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã®æœ€é©åŒ–
def optimize_torch_threads():
    """M2 Proç”¨ã«ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æœ€é©åŒ–"""
    # M2 Proã¯10ã‚³ã‚¢CPUï¼ˆ6æ€§èƒ½ + 4åŠ¹ç‡ï¼‰
    performance_cores = 6
    torch.set_num_threads(performance_cores)
    os.environ['OMP_NUM_THREADS'] = str(performance_cores)
    os.environ['MKL_NUM_THREADS'] = str(performance_cores)
    
# æœ€é©åŒ–4: MPSæœ€é©åŒ–è¨­å®š
def optimize_mps_settings():
    """Apple Silicon MPSç”¨ã®æœ€é©åŒ–è¨­å®š"""
    if torch.backends.mps.is_available():
        # MPSã®æœ€é©åŒ–è¨­å®š
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
        torch.mps.set_per_process_memory_fraction(0.75)  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ¶é™
        
# æœ€é©åŒ–5: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–
class OptimizedPipeline:
    def __init__(self, model_id):
        self.model_id = model_id
        self.pipeline = None
        self.device = None
        self.dtype = None
        
    def load(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            self.dtype = torch.float16  # MPSæœ€é©åŒ–: float16ä½¿ç”¨
        else:
            self.device = torch.device("cpu")
            self.dtype = torch.float32
            
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿
        self.pipeline = StableDiffusionPipeline.from_pretrained(
            self.model_id,
            torch_dtype=self.dtype,
            safety_checker=None,
            requires_safety_checker=False,
            use_safetensors=True,
            variant="fp16" if self.dtype == torch.float16 else None
        )
        
        # ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
        self.pipeline = self.pipeline.to(self.device)
        
        # æœ€é©åŒ–ã‚’é©ç”¨
        self._apply_optimizations()
        
    def _apply_optimizations(self):
        """å„ç¨®æœ€é©åŒ–ã‚’é©ç”¨"""
        if OPTIMIZATION_CONFIG['enable_attention_slicing']:
            self.pipeline.enable_attention_slicing(slice_size=1)
            
        if OPTIMIZATION_CONFIG['enable_vae_slicing']:
            self.pipeline.enable_vae_slicing()
            
        if OPTIMIZATION_CONFIG['enable_xformers']:
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
                print("âœ… xFormersæœ‰åŠ¹åŒ–æˆåŠŸ")
            except:
                print("âš ï¸ xFormersåˆ©ç”¨ä¸å¯")
                
        if OPTIMIZATION_CONFIG['use_channels_last']:
            # ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–
            self.pipeline.unet = self.pipeline.unet.to(memory_format=torch.channels_last)
            self.pipeline.vae = self.pipeline.vae.to(memory_format=torch.channels_last)
            
        if OPTIMIZATION_CONFIG['use_torch_compile'] and hasattr(torch, 'compile'):
            # PyTorch 2.0+ã®æœ€é©åŒ–
            self.pipeline.unet = torch.compile(self.pipeline.unet, mode="reduce-overhead")
            print("âœ… torch.compileæœ‰åŠ¹åŒ–æˆåŠŸ")
            
    def generate(self, **kwargs):
        """æœ€é©åŒ–ã•ã‚ŒãŸç”»åƒç”Ÿæˆ"""
        with torch.inference_mode():  # no_gradã‚ˆã‚Šé«˜é€Ÿ
            with torch.autocast("mps" if self.device.type == "mps" else "cpu"):
                return self.pipeline(**kwargs)
                
# æœ€é©åŒ–6: ç”»åƒå‡¦ç†ã®é«˜é€ŸåŒ–
def optimized_pixel_art_processing(image, pixel_size=8, palette_size=16):
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ”ã‚¯ã‚»ãƒ«ã‚¢ãƒ¼ãƒˆå‡¦ç†"""
    import cv2
    import numpy as np
    
    # PILã‹ã‚‰numpyã¸å¤‰æ›
    img_array = np.array(image)
    
    # OpenCVã§é«˜é€Ÿãƒªã‚µã‚¤ã‚º
    height, width = img_array.shape[:2]
    small_height = height // pixel_size
    small_width = width // pixel_size
    
    # ç¸®å°ï¼ˆINTER_NEARESTï¼‰
    small_img = cv2.resize(img_array, (small_width, small_height), interpolation=cv2.INTER_NEAREST)
    
    # ã‚«ãƒ©ãƒ¼é‡å­åŒ–ï¼ˆK-meansï¼‰
    if palette_size < 256:
        # Reshape for k-means
        pixels = small_img.reshape((-1, 3))
        pixels = np.float32(pixels)
        
        # K-means
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        _, labels, centers = cv2.kmeans(pixels, palette_size, None, criteria, 10, cv2.KMEANS_PP_CENTERS)
        
        # å†æ§‹ç¯‰
        centers = np.uint8(centers)
        quantized = centers[labels.flatten()]
        quantized = quantized.reshape(small_img.shape)
        small_img = quantized
    
    # æ‹¡å¤§
    pixel_art = cv2.resize(small_img, (width, height), interpolation=cv2.INTER_NEAREST)
    
    # PILã«æˆ»ã™
    return Image.fromarray(pixel_art)

# æœ€é©åŒ–7: ãƒ¡ãƒ¢ãƒªç®¡ç†
class MemoryManager:
    @staticmethod
    def cleanup():
        """ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        gc.collect()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        elif torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    @staticmethod
    def log_memory_usage():
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        import psutil
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: RSS={mem_info.rss/1024/1024:.1f}MB")

# ãƒ¡ã‚¤ãƒ³åˆæœŸåŒ–
if __name__ == "__main__":
    print("ğŸš€ Pixaæœ€é©åŒ–ç‰ˆã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­...")
    
    # æœ€é©åŒ–è¨­å®š
    optimize_torch_threads()
    optimize_mps_settings()
    
    # é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    lazy_import()
    
    print("âœ… æœ€é©åŒ–è¨­å®šå®Œäº†")
